import pickle
import json
import numpy as np
import tensorflow as tf
import csv
import os
from .config import Config


def preprocess(filepath, length=224):
    content = tf.read_file(filepath)
    thumbnail = preprocess_content(content, length)
    return thumbnail

def preprocess_content(content, length=224):
    raw = tf.decode_raw(content, tf.uint8)
    zero_padding_size = tf.constant(length) - tf.mod(tf.shape(raw), tf.constant(length))
    zero_padding = tf.zeros(zero_padding_size, dtype=tf.uint8)
    raw_padded = tf.concat(axis=0, values=[raw, zero_padding])
    image = tf.reshape(raw_padded, [-1, length, 1])
    thumbnail = image_preprocess(image, length)
    return thumbnail


def image_preprocess(image, length=224):
    thumbnail = tf.image.resize_images(image, [length, length],
                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    subval = 127.5
    divval = 127.5
    normalize = lambda x: tf.div(tf.subtract(tf.cast(x, tf.float32), subval), divval)
    thumbnail = normalize(thumbnail)
    return thumbnail


class IteratorInitializerHook(tf.train.SessionRunHook):

    def __init__(self):
        super(IteratorInitializerHook, self).__init__()
        self.iterator_initializer_func = None


    def after_create_session(self, session, coord):
        self.iterator_initializer_func(session)


class ThumbnailLoader(object):

    def __init__(self) :
        config = Config()
        self.config = config
        self.batch_size = config.batch_size
        self.num_epochs = config.num_epochs
        self.shuffle_buffer = config.shuffle_buffer
        self.labeldict_metadata_path = config.labeldict_path
        self.indexdict_metadata_path = config.indexdict_path
        self.tag_list_path = config.tag_list_path
        self.train_metadata_path = config.train_metadata_path
        self.valid_metadata_path = config.valid_metadata_path
        self.make_dicts_for_labels()


    def make_dicts_for_labels(self):
        raise NotImplementedError()


    def get_train_feature_dict(self):
        return self.get_feature_dict(self.train_metadata_path)


    def get_valid_feature_dict(self):
        return self.get_feature_dict(self.valid_metadata_path)


    def get_feature_dict(self, metadata_path):
        labels_arr = []
        filepath_arr = []
        index_arr = []

        with open(metadata_path, 'r') as f:
            reader = csv.DictReader(f)

            for index, row in enumerate(reader):
                path = row['path']
                labels = row['multitags']
                labels = self.get_label_for_purpose(labels)

                labels_arr.append(labels)
                filepath_arr.append(path)
                index_arr.append(index)

        filepath_arr = np.array(filepath_arr)
        labels_arr = np.array(labels_arr)
        index_arr = np.array(index_arr)
        print('data num : %d'%len(index_arr))

        return {'index_arr':index_arr, 'filepath_arr':filepath_arr,
                'labels_arr':labels_arr}


    def sparse_to_dense(self, sparse, label_num):
        dense = np.zeros(label_num)
        for index in sparse:
            dense[index] += 1
        return dense


    def select_features_and_labels(self):
        raise NotImplementedError()


    def get_train_iterator(self, dataset):
        dataset = dataset.prefetch(buffer_size=self.batch_size)
        dataset = dataset.repeat(self.num_epochs)
        dataset = dataset.shuffle(buffer_size=self.shuffle_buffer)
        dataset = dataset.batch(self.batch_size)
        iterator = dataset.make_initializable_iterator()
        return iterator


    def get_valid_iterator(self, dataset):
        dataset = dataset.prefetch(buffer_size=self.batch_size)
        dataset = dataset.repeat(1)
        dataset = dataset.shuffle(buffer_size=self.shuffle_buffer)
        dataset = dataset.batch(self.batch_size)
        iterator = dataset.make_initializable_iterator()
        return iterator


    def train_input_fn(self):
        iterator_initializer_hook = IteratorInitializerHook()
        def get_next():
            iterator = self.get_iterator(self.get_train_feature_dict(),
                                         self.get_train_iterator,
                                         iterator_initializer_hook)
            features, labels = iterator.get_next()
            return features, labels

        return get_next, iterator_initializer_hook



    def valid_input_fn(self):
        iterator_initializer_hook = IteratorInitializerHook()
        def get_next():
            iterator = self.get_iterator(self.get_valid_feature_dict(),
                                         self.get_valid_iterator,
                                         iterator_initializer_hook)
            features, labels = iterator.get_next()
            return features, labels

        return get_next, iterator_initializer_hook



    def predict_input_fn(self, filepath_arr):
        index_arr = np.arange(len(filepath_arr))
        def get_next():
            thumbnails = tf.data.Dataset.from_tensor_slices(filepath_arr).map(
            preprocess, num_parallel_calls=10)
            indices = tf.data.Dataset.from_tensor_slices(index_arr)
            dataset = tf.data.Dataset.zip(
            ({'x':thumbnails, 'idx':indices}))
            dataset = dataset.repeat(1)
            dataset = dataset.batch(1)
            iterator = dataset.make_one_shot_iterator()

            return iterator.get_next()
        return get_next


    def predict_content_input_fn(self, content_arr):
        index_arr = np.arange(len(content_arr))
        def get_next():
            thumbnails = tf.data.Dataset.from_tensor_slices(content_arr).map(
            preprocess_content, num_parallel_calls=10)
            indices = tf.data.Dataset.from_tensor_slices(index_arr)
            dataset = tf.data.Dataset.zip(
            ({'x':thumbnails, 'idx':indices}))
            dataset = dataset.repeat(1)
            dataset = dataset.batch(1)
            iterator = dataset.make_one_shot_iterator()

            return iterator.get_next()
        return get_next



    def get_iterator(self, dictionary, get_iterator, iterator_initializer_hook):

        index_arr = dictionary['index_arr']
        filepath_arr = dictionary['filepath_arr']
        labels_arr = dictionary['labels_arr']

        filepath_placeholder = tf.placeholder(
            filepath_arr.dtype, filepath_arr.shape, name='filepath_ph')
        index_placeholder = tf.placeholder(
            index_arr.dtype, index_arr.shape, name='index_ph')
        labels_placeholder = tf.placeholder(
            labels_arr.dtype, labels_arr.shape, name='labels_ph')

        thumbnails = tf.data.Dataset.from_tensor_slices(filepath_placeholder).map(
            preprocess, num_parallel_calls=10)
        indices = tf.data.Dataset.from_tensor_slices(index_placeholder)
        labels = tf.data.Dataset.from_tensor_slices(labels_placeholder)
        features, labels = self.select_features_and_labels(thumbnails, indices, labels)
        dataset = tf.data.Dataset.zip((features, labels))

        iterator = get_iterator(dataset)
        features, labels = iterator.get_next()

        iterator_initializer_hook.iterator_initializer_func = \
            lambda sess: sess.run(
                iterator.initializer,
                feed_dict={filepath_placeholder: filepath_arr,
                           index_placeholder: index_arr,
                           labels_placeholder: labels_arr})

        return iterator



    def get_dense_label(self, labels, label_num):
        labels_splitted = labels.split(' ')
        labels_sparse = [self.label_dict[label] for label in labels_splitted]
        labels_dense = self.sparse_to_dense(labels_sparse, label_num)
        return labels_dense


    def get_label_for_purpose(self, labels):
        raise NotImplementedError()



class ThumbnailMultilabelLoader(ThumbnailLoader):


    def make_dicts_for_labels(self):
        if os.path.exists(self.indexdict_metadata_path):
            with open(self.indexdict_metadata_path, 'rb') as f:
                self.index_dict = pickle.load(f)
        if os.path.exists(self.labeldict_metadata_path):
            with open(self.labeldict_metadata_path, 'rb') as f:
                self.label_dict = pickle.load(f)
            self.label_num = len(self.label_dict)
        return
        
        train_path = self.train_metadata_path
        valid_path = self.valid_metadata_path
        label_arr = []
        with open(train_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                labels = row['multitags']
                [label_arr.append(label) for label in labels.split(' ')]

        with open(valid_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                labels = row['multitags']
                [label_arr.append(label) for label in labels.split(' ')]

        labels, counts = np.unique(label_arr, return_counts=True)
        sorted_labels = list(map(lambda x:x[0], sorted(zip(labels, counts),
                                                       key=lambda x:x[1], reverse=True)))

        indexes = range(len(sorted_labels))

        self.label_dict = {str(label):index for label, index in zip(sorted_labels, indexes)}
        self.index_dict = {index:str(label) for label, index in zip(sorted_labels, indexes)}
        self.label_num = len(self.label_dict)

        with open(self.indexdict_metadata_path, 'wb') as f:
            pickle.dump(self.index_dict, f)

        with open(self.tag_list_path, 'w') as f:
            json.dump(self.index_dict, f)

        with open(self.labeldict_metadata_path, 'wb') as f:
            pickle.dump(self.label_dict, f)

        print('label_num : %d'%self.label_num)


    def select_features_and_labels(self, thumbnails, indices, labels):
        features = {'x':thumbnails, 'idx':indices}
        labels = labels
        return features, labels


    def get_label_for_purpose(self, labels):
        labels_dense = self.get_dense_label(labels, self.label_num)
        return labels_dense
